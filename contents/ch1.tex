\chapter{Introduction} \label{ch:introduction}

Neuro-symbolic learning methods and concepts have been used in recent years to achieve results that standalone deep learning and/or symbolic programming methods have not been able to achieve. These hybrid systems combine the pattern recognition and generalization capabilities of neural networks with the structured, rule-based reasoning of symbolic systems. For example, neuro-symbolic approaches have been successfully applied in visual question answering, knowledge base completion, and mathematical reasoning—areas where pure neural methods may struggle with logical consistency, and pure symbolic methods may lack adaptability (Besold et al., 2017; Mao et al., 2019). This synergy provides a framework for intelligent systems that are both robust and explainable.

The emerging developments in the field of neuro-symbolic learning have created opportunities to explore a wide range of applications and adaptations. Researchers have begun to integrate symbolic structures into neural models to improve generalization, transfer learning, and sample efficiency (Garcez et al., 2019). Conversely, symbolic reasoning systems are now being augmented with learned components to better handle ambiguity and noisy inputs. Such combinations have shown promise in domains like natural language understanding, program synthesis, and robotics, where both structured knowledge and perceptual learning are crucial (Evans \& Grefenstette, 2018). These developments suggest that hybrid approaches can fill critical gaps left by each paradigm alone.

Meanwhile, the field of Reinforcement Learning (RL) has made significant strides, demonstrating the paradigm’s effectiveness in creating agents that can autonomously perform complex tasks, such as playing strategic games, controlling robotic limbs, or navigating real-world environments (Mnih et al., 2015; Silver et al., 2017). RL agents learn through trial and error, developing sophisticated behavior through interactions with their environment. However, these agents often lack transparency in their decision-making processes, making them difficult to interpret and trust. This lack of interpretability limits their use in high-stakes or regulated domains where understanding and explaining behavior is essential.

The intersection of neuro-symbolic learning and reinforcement learning has opened new avenues for developing agents that are both high-performing and interpretable. By integrating symbolic reasoning into RL agents, it is possible to create systems that not only learn effective policies but also provide insights into their decision-making processes (Zambaldi et al., 2019). Ideally, a neuro-symbolic RL agent should be flexible in its ability to capture complex non-linear relationships while remaining interpretable in its reasoning and behavior. This blend can enable more reliable, transparent, and generalizable intelligent systems—paving the way for safer deployment in real-world environments, including healthcare, autonomous vehicles, and scientific discovery.

% Neuro-symbolic learning methods and concepts have been used in recent years to achieve results that standalone deep learning and/or symbolic programming methods have not been able to achieve. [EXAMPLES]. The emerging developments in the field of neuro-symbolic learning has created opportunities to explore applications and adaptations of these methods. The field of Reinforcement Learning (RL) has also made advances -- demonstrating the paradigms effectiveness in creating agents that can perform complex tasks autonomously. The intersection of these two research areas has led to developments that have allowed agents to perform tasks with with both programatic interpretability and learned performance. [EXAMPLES].
%
% Ideally a neuro-symbolic RL agent should be both flexible in it's ability to capture complex non-linear relationships and interpretable in its reasoning while acting in environment.


\section{Objectives} \label{se:objectives}
The primary objective of this thesis is to develop and evaluate a neuro-symbolic reinforcement learning (NSRL) framework that combines the perceptual capabilities of deep learning with the interpretability and structure of symbolic reasoning. The goal is to demonstrate that such a hybrid approach can yield agents that are both effective in complex task environments and more transparent in their decision-making processes. By integrating symbolic abstraction into the RL observation space, this work aims to improve the explainability, modularity, and control of learning agents—key properties that are often lacking in traditional deep reinforcement learning approaches.

This thesis proposes an adaptation of the Neuro-Symbolic Concept Learner (NS-CL) architecture, originally designed for visual question answering, for use within a reinforcement learning context. Rather than employing NS-CL purely as an output interpreter, the architecture is repurposed to act as a semantic filter that preprocesses raw sensory input into symbolic representations. These symbolic augmentations are then used to guide the learning process of the RL agent, allowing it to reason about tasks using interpretable, structured representations rather than unstructured pixel-level inputs.

Additionally, a secondary objective is to develop a robotic manipulation platform and simulation environment tailored to the evaluation of multi-task RL agents. This environment is designed to support symbolic task definitions and natural language prompts, providing a testbed for validating the generalization and flexibility of the proposed neuro-symbolic reinforcement learning framework. This enables the agent to perform real-world inspired tasks such as object sorting, placement, and navigation based on semantic goals.

Ultimately, this research aims to contribute a practical demonstration of neuro-symbolic integration within reinforcement learning and robotics. The outcome should highlight how symbolic reasoning can complement learned behavior, and offer insights into how structured knowledge can be used to improve safety, generalization, and usability in autonomous agents.

% The general aim of this thesis is to blend the concepts of neuro-symbolic learning with reinforcement learning to produce an agent that is both interpretable and flexible. More specifically, this thesis makes adaptations to the Neuro-symbolic Concept Learner Architecture leveraging it's ability to symbolically inference about a scene to inform an agent about its environment. By using the Concept Learner in this way, we are able to modify the observation state symbolically -- providing the engineer/user more control over the reinforcement learning agent.
%
% In addition to adapting Neuro-symbolic algorithms, this thesis also presents a robot manipulator platform and associated simulation environment designed with the intention of facilitating the verification and deployment of reinforcement learning algorithms.

\section{Applications} \label{se:applications}
Neuro-symbolic reinforcement learning has a wide range of potential applications, particularly in domains that require both perceptual learning and structured, interpretable decision-making. The integration of symbolic reasoning into reinforcement learning agents enhances their capacity to generalize across tasks, follow abstract instructions, and explain their actions—capabilities that are increasingly important in real-world systems. This section outlines key application areas, with a focus on robotic manipulation tasks in industrial settings.

\subsection{Assembly Line Automation}
One of the most promising applications of this work is in industrial automation, specifically in robotic assembly lines. Traditional automation systems rely on rigid, pre-programmed rules that often lack adaptability to dynamic environments. Neuro-symbolic RL agents, by contrast, can adapt to changing contexts while maintaining a symbolic understanding of tasks, objects, and goals. For example, an agent might be instructed to “place all red cubes in the bin,” and using symbolic augmentation, it can reason about object properties and execute appropriate actions—even when encountering variations in object placement or appearance. This level of semantic reasoning is crucial for enabling multi-task, reconfigurable robotic systems that reduce downtime and human oversight in manufacturing processes.

\subsection{Mulit-Task Robotic Control}
The ability to perform multiple tasks within a single environment is another major benefit of neuro-symbolic reinforcement learning. In traditional RL, agents are often trained for narrow, single-task performance. By contrast, a neuro-symbolic architecture allows for abstract task representations that can be shared and reused across different behaviors. For instance, an agent trained to “move the object to the left of the blue cylinder” can generalize that reasoning to a different object or spatial relationship using symbolic structures. This capability has implications for service robotics, warehouse automation, and assistive technologies, where flexible task understanding is vital.

\subsection{Natural Language Grounding}
Another application enabled by this architecture is the grounding of natural language commands into symbolic forms that inform reinforcement learning policies. This thesis demonstrates how natural language prompts can be interpreted by a symbolic module and used to condition the agent’s behavior. This has broad implications in human-robot interaction, where instructing a robot through high-level language—rather than low-level programming—is desirable. Applications could range from domestic robotics to collaborative manufacturing, where non-expert users need to communicate tasks to autonomous systems in an intuitive and interpretable way.

\section{Scope}
\section{Contributions}
